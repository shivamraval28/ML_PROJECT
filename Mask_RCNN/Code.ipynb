{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nightly in c:\\users\\shiva\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.10.0.dev2022041907)\n",
      "Found existing installation: tensorflow 2.8.0\n",
      "Uninstalling tensorflow-2.8.0:\n",
      "  Successfully uninstalled tensorflow-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\commands\\uninstall.py\", line 102, in run\n",
      "    uninstall_pathset.commit()\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 420, in commit\n",
      "    self._moved_paths.commit()\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 273, in commit\n",
      "    save_dir.cleanup()\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\utils\\temp_dir.py\", line 173, in cleanup\n",
      "    rmtree(self._path)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 326, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 406, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 362, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 195, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 409, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 124, in rmtree\n",
      "    shutil.rmtree(dir, ignore_errors=ignore_errors, onerror=rmtree_errorhandler)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\", line 747, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\", line 612, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\", line 612, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\", line 612, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\", line 617, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py\", line 615, in _rmtree_unsafe\n",
      "    os.unlink(fullname)\n",
      "PermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\shiva\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\~ensorflow\\\\lite\\\\experimental\\\\microfrontend\\\\python\\\\ops\\\\_audio_microfrontend_op.so'\n",
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-nightly\n",
    "!pip uninstall -y tensorflow\n",
    "%tensorflow_version 1.x\n",
    "!pip install --upgrade h5py==2.10.0\n",
    "!pip install -U scikit-image==0.16.2\n",
    "!pip install tensorflow \n",
    "import sys\n",
    "sys.path.append(\"/content/Mask_RCNN\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\shiva\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"G:\\Git\\ML_Project\\Dataset\\ML_Project\\VOC2007\"\n",
    "annotations = \"G:\\Git\\ML_Project\\Dataset\\ML_Project\\VOC2007\\Annotations\"\n",
    "images = \"G:\\Git\\ML_Project\\Dataset\\ML_Project\\VOC2007\\JPEGImages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create Custom Dataset field for loading our Pascal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mrcnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000005untitled?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmrcnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000005untitled?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPascalDataset\u001b[39;00m(Dataset):\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000005untitled?line=3'>4</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000005untitled?line=4'>5</a>\u001b[0m \u001b[39m    mainly three methods, load_datset, load_masks and image_reference needs to be modified according to our dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000005untitled?line=5'>6</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mrcnn'"
     ]
    }
   ],
   "source": [
    "from mrcnn.utils import Dataset\n",
    "\n",
    "class PascalDataset(Dataset):\n",
    "    '''\n",
    "    mainly three methods, load_datset, load_masks and image_reference needs to be modified according to our dataset\n",
    "    '''\n",
    "\n",
    "    def load_dataset(self, txt_file ,dataset_dir):\n",
    "\n",
    "      #adding all classes from Pascal Dataset\n",
    "        self.add_class(\"dataset\",1,'aeroplane')\n",
    "        self.add_class(\"dataset\",2,'bicycle')\n",
    "        self.add_class(\"dataset\",3,'bird')\n",
    "        self.add_class(\"dataset\",4,'boat')\n",
    "        self.add_class(\"dataset\",5,'bottle')\n",
    "        self.add_class(\"dataset\",6,'bus')\n",
    "        self.add_class(\"dataset\",7,'car')\n",
    "        self.add_class(\"dataset\",8,'cat')\n",
    "        self.add_class(\"dataset\",9,'chair')\n",
    "        self.add_class(\"dataset\",10,'cow')\n",
    "        self.add_class(\"dataset\",11,'diningtable')\n",
    "        self.add_class(\"dataset\",12,'dog')\n",
    "        self.add_class(\"dataset\",13,'horse')\n",
    "        self.add_class(\"dataset\",14,'motorbike')\n",
    "        self.add_class(\"dataset\",15,'person')\n",
    "        self.add_class(\"dataset\",16,'pottedplant')\n",
    "        self.add_class(\"dataset\",17,'sheep')\n",
    "        self.add_class(\"dataset\",18,'sofa')\n",
    "        self.add_class(\"dataset\",19,'train')\n",
    "        self.add_class(\"dataset\",20,'tvmonitor')\n",
    "\n",
    "        # define data locations\n",
    "        images_dir = dataset_dir + '/JPEGImages/'\n",
    "        annotations_dir = dataset_dir + '/Annotations/'\n",
    "\n",
    "        with open(txt_file) as f:\n",
    "          all_images = f.read().split('\\n')\n",
    "\n",
    "        # Split the dataset, if train, get 90%, else 10%\n",
    "        # len_images = len(os.listdir(images_dir))\n",
    "        # if is_train == \"train\":\n",
    "        #     img_range = [int(len_images / 9), len_images]\n",
    "        # else:\n",
    "        #     img_range = [0, int(len_images / 9)]\n",
    "\n",
    "        # find all images\n",
    "        # for i in range(img_range[0],img_range[1]):\n",
    "        # for filename in os.listdir(images_dir):\n",
    "        for image_id in all_images:\n",
    "            \n",
    "            # extract image id\n",
    "            # image_id = filename\n",
    "            # skip bad images\n",
    "            \n",
    "            img_path = images_dir + image_id + '.jpg'\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "            \n",
    "            # check if is_train is true or false based on that decide if it'll be in train_dataset\n",
    "            \n",
    "            # add to dataset\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    def extract_boxes(self,filename):\n",
    "      '''\n",
    "      extracting all the bounding boxes avaialable in the given image\n",
    "      '''\n",
    "      # load and parse the file\n",
    "      tree = ET.parse(filename)\n",
    "      # get the root of the document\n",
    "      root = tree.getroot()\n",
    "      # extract each bounding box\n",
    "      objects = list() #list of tuples (name,box_coors)\n",
    "      for obj in root.findall('.//object'):\n",
    "          xmin = int(obj.find('./bndbox/xmin').text)\n",
    "          ymin = int(obj.find('./bndbox/ymin').text)\n",
    "          xmax = int(obj.find('./bndbox/xmax').text)\n",
    "          ymax = int(obj.find('./bndbox/ymax').text)\n",
    "          name = obj.find('./name').text\n",
    "          coors = [xmin, ymin, xmax, ymax]\n",
    "          objects.append((name,coors))\n",
    "      # extract image dimensions\n",
    "      width = int(root.find('.//size/width').text)\n",
    "      height = int(root.find('.//size/height').text)\n",
    "      return objects, width, height\n",
    "  \n",
    "    #load mask for an image\n",
    "    def load_mask(self, image_id):\n",
    "        # get details of image\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # define box file location\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # load XML\n",
    "        objects, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # create one array for all masks, each on a different channel\n",
    "        masks = np.zeros([h, w, len(objects)], dtype='uint8')\n",
    "        \n",
    "        # create masks\n",
    "        class_ids = list()\n",
    "        for i in range(len(objects)):\n",
    "            name, box = objects[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index(name))\n",
    "        return masks, np.asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    #load an Image referance\n",
    "    def image_referance(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7679b3f13171ca1df491d4b5b53694221155d4305fc1b0a8de727533012e72db"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
